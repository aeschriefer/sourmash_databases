# first, gather all '.fna.gz'
import os
import random

configfile: "config.yml"

prefix = config.get('prefix', '')
genomes_location = config['genomes_location']
sigs_output_location = config['sigs_output_location']

all_files = []
for root, dirs, files in os.walk(genomes_location, topdown=False):
    for name in files:
        if name.endswith('_genomic.fna.gz'):
            filename = os.path.join(root, name)
            if filename.startswith('./'): filename = filename[2:]
            all_files.append(filename)

print('found {} files under {}'.format(len(all_files), genomes_location))
random.shuffle(all_files)
print('examples:', all_files[:5])

def make_sigfile(filename):
    genomefile = os.path.basename(filename)
    sigfile = os.path.join(sigs_output_location, genomefile) + '.sig'

    return sigfile

# rule all: make sigs
rule all:
    input:
        [ make_sigfile(genomefile) for genomefile in all_files ],
        prefix+"gtdb-release89-k21.lca.json.gz",
        prefix+"gtdb-release89-k31.lca.json.gz",
        prefix+"gtdb-release89-k51.lca.json.gz",
        prefix+"gtdb-release89-k21.sbt.json",
        prefix+"gtdb-release89-k31.sbt.json",
        prefix+"gtdb-release89-k51.sbt.json"

rule make_lineage_csv:
    input:
        config['gtdb_taxonomy_tsv']
    output:
        'gtdb-lineages.csv'
    shell:
        '../update-gtdb-taxonomy.py {input} -o {output}'

rule make_lca_db:
    input:
        "gtdb-lineages.csv",
        [ make_sigfile(genomefile) for genomefile in all_files ]
    output:
        prefix+"gtdb-release89-k{ksize}.lca.json.gz"
    params:
        ksize="{ksize}",
        prefix=prefix
    shell:
        """sourmash lca index -k {params.ksize} \
                           --scaled 10000 --require-taxonomy \
                           --split-identifiers \
                           --report report-gtdb-k31.txt \
                           --traverse-directory -C 3 \
                           gtdb-lineages.csv \
                           {params.prefix}gtdb-release89-k{params.ksize}.lca.json.gz {sigs_output_location}
        """

rule make_sbt_db:
    input:
        [ make_sigfile(genomefile) for genomefile in all_files ]
    output:
        prefix+"gtdb-release89-k{ksize}.sbt.json"
    params:
        ksize="{ksize}",
        prefix=prefix
    shell:
        """sourmash index -k {params.ksize} \
                           --traverse-directory \
                           {params.prefix}gtdb-release89-k{params.ksize}.sbt.json {sigs_output_location}
        """

filename_to_names = None
def lookup_name(filename):
    global filename_to_names

    if filename_to_names is None:
        import csv
        d = {}
        with open('gtdb-lineages.csv', 'rt') as fp:
            r = csv.DictReader(fp)
            for row in r:
                filename = row['filename']
                name = "{} {}".format(row['accession'], row['species'])
                d[filename] = name
        filename_to_names = d

    key = os.path.basename(filename)
    print('looking up', key)
    return filename_to_names[key]

# generic rule: compute signature
rule compute_sig:
    input:
        os.path.join(genomes_location, "{filename}"),
        'gtdb-lineages.csv'
    output:
        os.path.join(sigs_output_location + "{filename}.sig")
    params:
        signame = lambda wildcards: lookup_name(wildcards.filename)
    shell:
        "sourmash compute -k 21,31,51 --scaled=2000 {input[0]} -o {output} --merge={params.signame:q}"
